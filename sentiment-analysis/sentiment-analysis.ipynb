{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  # generate random numbers and manipulate random selections\n",
    "import numpy as np  # numerical computing library\n",
    "import pandas as pd  # manipulatin data structures and data analysis tools\n",
    "import torch  # tensor operations and neural network computation\n",
    "from torch import optim  # optimizing parameter on training neural networks\n",
    "import torch.nn.functional as F  # neural network functions like activation and loss functions\n",
    "from tqdm import tqdm  # show progress bar for loops\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer  # Provides pre-trained BERT models, configuration, and tokenization utilities\n",
    "from nltk.tokenize import TweetTokenizer  # tweet tokenizer\n",
    "\n",
    "from indonlu.utils.forward_fn import forward_sequence_classification  # forward pass in sequence classification tasks\n",
    "from indonlu.utils.metrics import document_sentiment_metrics_fn  # evaluating document sentiment analysis\n",
    "from indonlu.utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader  # handling datasets and data loaders for document sentiment tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA tersedia!\n",
      "Device name: NVIDIA GeForce RTX 2050\n",
      "Jumlah device: 1\n"
     ]
    }
   ],
   "source": [
    "# CEK CUDA\n",
    "def cek_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA tersedia!\")\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Jumlah device: {torch.cuda.device_count()}\")\n",
    "    else:\n",
    "        print(\"CUDA tidak tersedia.\")\n",
    "\n",
    "cek_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COMMON FUNCTION\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# set and assign random seed.\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# count number of parameters in a model    \n",
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "# set learning rate     \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# convert metrics to string \n",
    "def metrics_to_string(metric_dict):\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append('{}:{:.2f}'.format(key, value))\n",
    "    return ' '.join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(20240802)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Config\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config.num_labels = DocumentSentimentDataset.NUM_LABELS\n",
    " \n",
    "# Instantiate model\n",
    "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124443651"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_param(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = 'F:/dicoding-ml-terapan/dicoding-ml-terapan/sentiment-analysis/indonlu/dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv'\n",
    "valid_dataset_path = 'F:/dicoding-ml-terapan/dicoding-ml-terapan/sentiment-analysis/indonlu/dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv'\n",
    "test_dataset_path = 'F:/dicoding-ml-terapan/dicoding-ml-terapan/sentiment-analysis/indonlu/dataset/smsa_doc-sentiment-prosa/test_preprocess_masked_label.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ubaid\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\n",
    "valid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
    "test_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\n",
    " \n",
    "train_loader = DocumentSentimentDataLoader(dataset=train_dataset, \n",
    "                                           max_seq_len=512, \n",
    "                                           batch_size=32, \n",
    "                                           num_workers=16, \n",
    "                                           shuffle=True)  \n",
    "valid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, \n",
    "                                           max_seq_len=512, \n",
    "                                           batch_size=32, \n",
    "                                           num_workers=16, \n",
    "                                           shuffle=False)  \n",
    "test_loader = DocumentSentimentDataLoader(dataset=test_dataset, \n",
    "                                          max_seq_len=512, \n",
    "                                          batch_size=32, \n",
    "                                          num_workers=16, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([    2,  6540,    92,  2970,   213,  4259,  3553,   899,    34,\n",
      "         259,  5590,   262,  2558,   386,   899,  1687,    26,  1574,\n",
      "       30470,   899,  3310, 30468, 22130, 30360,  6123,  6368, 30468,\n",
      "       22130, 30360,  2652,  1746, 30468,  8869,  6540,    34,  6315,\n",
      "        1622,  1256,  8949,   899, 30468,  4222,  1622,   752,   245,\n",
      "         295,  2083, 30470,  2346,  7107,   300, 30470,   405,   724,\n",
      "        5189, 30470,   843, 17464,   899,   540, 10989,  3331,  1107,\n",
      "       30468,   119,  3221,    79,    34,  2170,    98,  9167, 30457,\n",
      "           3]), array(0, dtype=int64), 'warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !')\n",
      "<indonlu.utils.data_utils.DocumentSentimentDataLoader object at 0x000001CB5D615720>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 0, 'neutral': 1, 'negative': 2}\n",
      "{0: 'positive', 1: 'neutral', 2: 'negative'}\n"
     ]
    }
   ],
   "source": [
    "label_to_index, index_to_label = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "print(label_to_index)\n",
    "print(index_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita | Label : positive (37.769%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    " \n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    " \n",
    "print(f'Text: {text} | Label : {index_to_label[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params=model.parameters(), lr=3e-6)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/344 [00:00<?, ?it/s]c:\\Users\\ubaid\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "c:\\Users\\ubaid\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "(EPOCH 1) TRAIN LOSS:0.3474 LR:0.00000300: 100%|██████████| 344/344 [09:15<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(| EPOCH 1) TRAIN LOSS:0.3474 ACC:0.87 F1:0.82 REC:0.80 PRE:0.85 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]c:\\Users\\ubaid\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "VALID LOSS:0.1973 ACC:0.93 F1:0.89 REC:0.87 PRE:0.92: 100%|██████████| 40/40 [00:57<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(| EPOCH 1) VALID LOSS:0.1973 ACC:0.93 F1:0.89 REC:0.87 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/344 [00:00<?, ?it/s]c:\\Users\\ubaid\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "(EPOCH 2) TRAIN LOSS:0.1599 LR:0.00000300: 100%|██████████| 344/344 [09:20<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(| EPOCH 2) TRAIN LOSS:0.1599 ACC:0.95 F1:0.93 REC:0.92 PRE:0.94 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]c:\\Users\\ubaid\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "VALID LOSS:0.1720 ACC:0.93 F1:0.90 REC:0.90 PRE:0.90: 100%|██████████| 40/40 [00:57<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(| EPOCH 2) VALID LOSS:0.1720 ACC:0.93 F1:0.90 REC:0.90 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/344 [00:00<?, ?it/s]c:\\Users\\ubaid\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "(EPOCH 3) TRAIN LOSS:0.1130 LR:0.00000300:  16%|█▌        | 54/344 [01:59<07:40,  1.59s/it] "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "n_epochs = 3\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    total_train_loss = 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    train_progressbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "\n",
    "    for i, batch_data in enumerate(train_progressbar):\n",
    "        # Forward model\n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model=model, \n",
    "                                                                       batch_data=batch_data[:-1],\n",
    "                                                                       i2w=index_to_label,\n",
    "                                                                       device='cuda')\n",
    "        \n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss = loss.item()\n",
    "        total_train_loss = total_train_loss + train_loss\n",
    "\n",
    "        # Calculate metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "\n",
    "        train_progressbar.set_description(\"(EPOCH {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format(epoch+1,\n",
    "                                                                                          total_train_loss/(i+1),\n",
    "                                                                                          get_lr(optimizer)))\n",
    "    \n",
    "    # Calculate train metric\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(| EPOCH {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
    "                                                               total_train_loss/(i+1),\n",
    "                                                               metrics_to_string(metrics), \n",
    "                                                               get_lr(optimizer)))\n",
    " \n",
    "    # Evaluate on validation\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    " \n",
    "    progressbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
    "    for i, batch_data in enumerate(progressbar):\n",
    "        batch_seq = batch_data[-1]        \n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model, \n",
    "                                                                       batch_data[:-1], \n",
    "                                                                       i2w=index_to_label, \n",
    "                                                                       device='cuda')\n",
    "        \n",
    "        # Calculate total loss\n",
    "        valid_loss = loss.item()\n",
    "        total_loss = total_loss + valid_loss\n",
    " \n",
    "        # Calculate evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    " \n",
    "        progressbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "        \n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(| EPOCH {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
    "                                                     total_loss/(i+1), \n",
    "                                                     metrics_to_string(metrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    " \n",
    "total_loss, total_correct, total_labels = 0, 0, 0\n",
    "list_hyp, list_label = [], []\n",
    " \n",
    "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "for i, batch_data in enumerate(pbar):\n",
    "    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=index_to_label, device='cuda')\n",
    "    list_hyp += batch_hyp\n",
    " \n",
    "# Save prediction\n",
    "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "df.to_csv('pred.txt', index=False)\n",
    " \n",
    "print(df)\n",
    "# Evaluate on test\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    " \n",
    "total_loss, total_correct, total_labels = 0, 0, 0\n",
    "list_hyp, list_label = [], []\n",
    " \n",
    "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "for i, batch_data in enumerate(pbar):\n",
    "    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=index_to_label, device='cuda')\n",
    "    list_hyp += batch_hyp\n",
    " \n",
    "# Save prediction\n",
    "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "df.to_csv('pred.txt', index=False)\n",
    " \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    " \n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    " \n",
    "print(f'Text: {text} | Label : {index_to_label[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teks</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Teks    Target\n",
       "0  warung ini dimiliki oleh pengusaha pabrik tahu...  positive\n",
       "1  mohon ulama lurus dan k212 mmbri hujjah partai...   neutral\n",
       "2  lokasi strategis di jalan sumatera bandung . t...  positive\n",
       "3  betapa bahagia nya diri ini saat unboxing pake...  positive\n",
       "4  duh . jadi mahasiswa jangan sombong dong . kas...  negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(train_dataset_path, sep='\\t', names=['Teks', 'Target'])\n",
    "data_valid= pd.read_csv(test_dataset_path, sep='\\t', names=['Teks', 'Target'])\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shhfhss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
